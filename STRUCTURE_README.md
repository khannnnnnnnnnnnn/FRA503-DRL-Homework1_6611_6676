# Output Structure Documentation

This document explains the organized output structure generated by the multi-armed bandit experiments.

## Directory Structure

After running `python simulation.py`, the following directory structure is created:

```
.
├── figures/                          # All visualization outputs
│   ├── epsilon_greedy_eps0.0/       # Individual experiment results
│   │   ├── q_comparison.png         # Learned vs true Q-values
│   │   ├── reward_distribution.png  # Reward distribution over time
│   │   ├── action_counts.png        # How many times each action was selected
│   │   └── q_error.png             # Q-value estimation error / regret
│   ├── epsilon_greedy_eps0.01/
│   │   └── ... (same 4 plots)
│   ├── epsilon_greedy_eps0.05/
│   │   └── ...
│   ├── epsilon_greedy_eps0.1/
│   │   └── ...
│   ├── epsilon_greedy_eps0.2/
│   │   └── ...
│   ├── epsilon_greedy_eps0.3/
│   │   └── ...
│   ├── ucb_c0.5/
│   │   └── ... (same 4 plots)
│   ├── ucb_c1.0/
│   │   └── ...
│   ├── ucb_c2.0/
│   │   └── ...
│   ├── ucb_c3.0/
│   │   └── ...
│   ├── ucb_c5.0/
│   │   └── ...
│   ├── combined_rewards.png         # All configurations reward comparison
│   ├── combined_optimal.png         # All configurations optimal action rate
│   ├── combined_regret.png          # All configurations regret comparison
│   ├── combined_q_error.png         # All configurations Q-error comparison
│   ├── group_epsilon_greedy.png     # 2x2 summary plot for epsilon-greedy
│   ├── group_ucb.png                # 2x2 summary plot for UCB
│   └── best_overlay.png             # Best epsilon-greedy vs best UCB
│
└── logs/                             # All numerical data and logs
    ├── epsilon_greedy_eps0.0/
    │   ├── epsilon_greedy_eps0.0_log.json      # Metadata and summary statistics
    │   └── epsilon_greedy_eps0.0_results.csv   # Detailed timestep-by-timestep data
    ├── epsilon_greedy_eps0.01/
    │   └── ... (same 2 files)
    ├── epsilon_greedy_eps0.05/
    │   └── ...
    ├── epsilon_greedy_eps0.1/
    │   └── ...
    ├── epsilon_greedy_eps0.2/
    │   └── ...
    ├── epsilon_greedy_eps0.3/
    │   └── ...
    ├── ucb_c0.5/
    │   └── ... (same 2 files)
    ├── ucb_c1.0/
    │   └── ...
    ├── ucb_c2.0/
    │   └── ...
    ├── ucb_c3.0/
    │   └── ...
    └── ucb_c5.0/
        └── ...
```

## File Descriptions

### Individual Experiment Plots (4 per configuration)

Each experiment configuration (e.g., `epsilon_greedy_eps0.1/`) contains:

1. **`q_comparison.png`**
   - Bar chart comparing learned Q-values vs true bandit values
   - Shows how well the algorithm estimated each arm's value
   - Includes error bars showing standard deviation across runs

2. **`reward_distribution.png`**
   - Line plot of average reward over time (smoothed)
   - Shaded region shows ±1 standard deviation
   - Demonstrates learning progress and stability

3. **`action_counts.png`**
   - Bar chart of total action selections
   - Shows which arms were pulled most often
   - Helps identify exploration vs exploitation balance

4. **`q_error.png`**
   - Average regret per step over time
   - Shows how quickly the algorithm approaches optimal performance
   - Lower is better

### Combined Comparison Plots (in `figures/` root)

1. **`combined_rewards.png`**
   - All parameter configurations overlaid
   - Average reward over time for each setting
   - Helps identify which parameter value performs best

2. **`combined_optimal.png`**
   - Optimal action selection rate over time
   - Shows convergence speed for each configuration
   - Target is 100% optimal action selection

3. **`combined_regret.png`**
   - Cumulative regret for all configurations
   - Shows total opportunity cost of exploration
   - Lower curves are better

4. **`combined_q_error.png`**
   - Average regret per step for all configurations
   - Normalized view of performance efficiency
   - Flatter curves indicate stable learning

### Group Summary Plots

1. **`group_epsilon_greedy.png`**
   - 2×2 grid showing all key metrics for epsilon-greedy
   - Comprehensive overview in one image
   - Subplots: Average Reward, Cumulative Reward, Optimal %, Regret

2. **`group_ucb.png`**
   - Same 2×2 layout for UCB algorithm
   - Easy algorithm comparison side-by-side

3. **`best_overlay.png`**
   - Direct comparison of best epsilon-greedy vs best UCB
   - 2×2 grid with both algorithms overlaid
   - Final head-to-head performance evaluation

### Log Files

#### JSON Log (`*_log.json`)

Contains experiment metadata and summary statistics:

```json
{
  "experiment_name": "epsilon_greedy_eps0.1",
  "algorithm": "epsilon-greedy",
  "parameter": {
    "name": "epsilon",
    "value": 0.1
  },
  "configuration": {
    "n_runs": 100,
    "n_steps": 10000
  },
  "summary_statistics": {
    "final_cumulative_reward": {
      "mean": 12500.45,
      "std": 125.32
    },
    "final_regret": {
      "mean": 1250.20,
      "std": 98.15
    },
    ...
  }
}
```

#### CSV Results (`*_results.csv`)

Timestep-by-timestep data for detailed analysis:

| Column | Description |
|--------|-------------|
| `step` | Timestep number (0 to n_steps-1) |
| `reward_mean` | Average reward at this timestep |
| `reward_std` | Standard deviation of reward |
| `cumulative_reward_mean` | Mean cumulative reward up to this point |
| `cumulative_reward_std` | Std of cumulative reward |
| `optimal_action_mean` | Fraction of runs selecting optimal action |
| `optimal_action_std` | Std of optimal action selection |
| `regret_mean` | Mean cumulative regret |
| `regret_std` | Std of cumulative regret |

## Usage Examples

### Loading and Analyzing Results

```python
import json
import pandas as pd
import matplotlib.pyplot as plt

# Load JSON metadata
with open('logs/epsilon_greedy_eps0.1/epsilon_greedy_eps0.1_log.json', 'r') as f:
    metadata = json.load(f)

print(f"Final reward: {metadata['summary_statistics']['final_cumulative_reward']['mean']:.2f}")

# Load CSV for detailed analysis
df = pd.read_csv('logs/epsilon_greedy_eps0.1/epsilon_greedy_eps0.1_results.csv')

# Plot custom analysis
plt.figure(figsize=(10, 6))
plt.plot(df['step'], df['reward_mean'])
plt.fill_between(df['step'], 
                 df['reward_mean'] - df['reward_std'],
                 df['reward_mean'] + df['reward_std'], 
                 alpha=0.3)
plt.xlabel('Timestep')
plt.ylabel('Reward')
plt.title('Custom Reward Analysis')
plt.show()
```

### Comparing Multiple Configurations

```python
import pandas as pd
import matplotlib.pyplot as plt

configs = ['epsilon_greedy_eps0.01', 'epsilon_greedy_eps0.1', 'epsilon_greedy_eps0.3']
colors = ['blue', 'green', 'red']

plt.figure(figsize=(12, 6))
for config, color in zip(configs, colors):
    df = pd.read_csv(f'logs/{config}/{config}_results.csv')
    plt.plot(df['step'], df['cumulative_reward_mean'], label=config, color=color)

plt.xlabel('Timestep')
plt.ylabel('Cumulative Reward')
plt.legend()
plt.title('Cumulative Reward Comparison')
plt.show()
```

## Configuration Options

### Tested Parameters

**Epsilon-Greedy (`epsilon_values`):**
- `0.0` - Pure greedy (no exploration)
- `0.01` - Minimal exploration
- `0.05` - Low exploration
- `0.1` - Moderate exploration (often optimal)
- `0.2` - High exploration
- `0.3` - Very high exploration

**UCB (`ucb_c_values`):**
- `0.5` - Conservative exploration
- `1.0` - Moderate exploration
- `2.0` - Standard exploration (often optimal)
- `3.0` - Aggressive exploration
- `5.0` - Very aggressive exploration

### Experimental Settings

Default configuration in `simulation.py`:

```python
N_BANDITS = 10      # Number of arms
N_STEPS = 10000     # Timesteps per run
N_RUNS = 100        # Independent runs (for statistics)
```

Modify these in the script to adjust:
- Problem complexity (more/fewer arms)
- Learning duration (more/fewer steps)
- Statistical confidence (more/fewer runs)

## Statistical Significance

All results represent:
- **Mean**: Average across N_RUNS independent trials
- **Std**: Standard deviation showing variability
- **Confidence**: 100 runs provide ~95% confidence intervals when std is shown

Shaded regions in plots typically represent ±1 standard deviation.

## Quick Reference

### Best Practices

1. **For quick results**: Reduce N_RUNS to 10-20
2. **For publication**: Keep N_RUNS at 100+
3. **For long experiments**: Increase N_STEPS
4. **For comparison**: Focus on `best_overlay.png` and group plots
5. **For analysis**: Use CSV files for custom metrics

### File Sizes

Approximate sizes with default settings:
- Each PNG: 100-500 KB
- Each JSON: 1-5 KB
- Each CSV: 500 KB - 2 MB
- Total: ~50-100 MB for full experiment suite

## Troubleshooting

**Missing figures/logs directories:**
- Automatically created on first run
- If deleted, will be recreated

**Incomplete results:**
- Check console output for errors
- Verify N_RUNS completed successfully
- Check disk space

**Performance issues:**
- Reduce N_RUNS for testing
- Reduce N_STEPS for faster iteration
- Use fewer parameter values

## Citation

If using this framework in academic work, please cite:
- Sutton & Barto (2018), Reinforcement Learning: An Introduction
- FRA 503 course materials

---

*Generated by Multi-Armed Bandit Framework v1.0*
*For questions: See README.md or QUICKSTART.md*
